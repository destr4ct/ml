import torch
import numpy as np


class Module:
    """
    Basically, you can think of a module as of a something (black box) 
    which can process `input` data and produce `ouput` data.
    This is like applying a function which is called `forward`: 

        output = module.forward(input)

    The module should be able to perform a backward pass: to differentiate the `forward` function. 
    More, it should be able to differentiate it if is a part of chain (chain rule).
    The latter implies there is a gradient from previous step of a chain rule. 

        input_grad = module.backward(input, output_grad)
    """

    def __init__(self):
        self._output = None
        self._input_grad = None
        self.training = True

    def forward(self, input):
        """
        Takes an input object, and computes the corresponding output of the module.
        """
        self._output = self._compute_output(input)
        return self._output

    def backward(self, input, output_grad):
        """
        Performs a backpropagation step through the module, with respect to the given input.

        This includes 
         - computing a gradient w.r.t. `input` (is needed for further backprop),
         - computing a gradient w.r.t. parameters (to update parameters while optimizing).
        """
        self._input_grad = self._compute_input_grad(input, output_grad)
        self._update_parameters_grad(input, output_grad)
        return self._input_grad

    def _compute_output(self, input):
        """
        Computes the output using the current parameter set of the class and input.
        This function returns the result which will be stored in the `_output` field.

        Example: in case of identity operation:

        output = input 
        return output
        """
        raise NotImplementedError

    def _compute_input_grad(self, input, output_grad):
        """
        Returns the gradient of the module with respect to its own input. 
        The shape of the returned value is always the same as the shape of `input`.

        Example: in case of identity operation:
        input_grad = output_grad
        return input_grad
        """

        raise NotImplementedError

    def _update_parameters_grad(self, input, output_grad):
        """
        Computing the gradient of the module with respect to its own parameters.
        No need to override if module has no parameters (e.g. ReLU).
        """
        pass

    def zero_grad(self): 
        """
        Zeroes `gradParams` variable if the module has params.
        """
        pass

    def get_parameters(self):
        """
        Returns a list with its parameters. 
        If the module does not have parameters return empty list. 
        """
        return []

    def get_parameters_grad(self):
        """
        Returns a list with gradients with respect to its parameters. 
        If the module does not have parameters return empty list. 
        """
        return []

    def train(self):
        """
        Sets training mode for the module.
        Training and testing behaviour differs for Dropout, BatchNorm.
        """
        self.training = True

    def evaluate(self):
        """
        Sets evaluation mode for the module.
        Training and testing behaviour differs for Dropout, BatchNorm.
        """
        self.training = False

    def __repr__(self):
        """
        Pretty printing. Should be overrided in every module if you want 
        to have readable description. 
        """
        return "Module"


class Sequential(Module):
    """
    This class implements a container, which processes `input` data sequentially. 

    `input` is processed by each module (layer) in self.modules consecutively.
    The resulting array is called `_output`. 
    """

    def __init__(self):
        super(Sequential, self).__init__()
        self.modules = []

    def add_module(self, module):
        """
        Adds a module to the container.
        """
        self.modules.append(module)

    def _compute_output(self, input):
        """
        Basic workflow of FORWARD PASS:

            y_0    = module[0].forward(input)
            y_1    = module[1].forward(y_0)
            ...
            output = module[n-1].forward(y_{n-2})   

        """

        y = input
        for cm in self.modules:
            y = cm.forward(y)
        return y

    def _compute_input_grad(self, input, output_grad):
        """
        Workflow of BACKWARD PASS:

            g_{n-1} = module[n-1].backward(y_{n-2}, output_grad)
            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})
            ...
            g_1 = module[1].backward(y_0, g_2)   
            grad_input = module[0].backward(input, g_1)   


        get_ipython().getoutput("!!")

        To each module you need to provide the input, module saw while forward pass, 
        it is used while computing gradients. 
        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) 
        and NOT `input` to this Sequential module. 

        get_ipython().getoutput("!!")

        """

        gi = output_grad
        for m in reversed(self.modules):
            gi = m.backward(m._output, gi)
        return gi

    def zero_grad(self):
        for module in self.modules:
            module.zero_grad()

    def get_parameters(self):
        """
        Should gather all parameters in a list.
        """
        return [x.get_parameters() for x in self.modules]

    def get_parameters_grad(self):
        """
        Should gather all gradients w.r.t parameters in a list.
        """
        return [x.get_parameters_grad() for x in self.modules]

    def __repr__(self):
        string = "".join([str(x) + '\n' for x in self.modules])
        return string

    def __getitem__(self, x):
        return self.modules.__getitem__(x)

    def train(self):
        """
        Propagates training parameter through all modules
        """
        self.training = True
        for module in self.modules:
            module.train()

    def evaluate(self):
        """
        Propagates training parameter through all modules
        """
        self.training = False
        for module in self.modules:
            module.evaluate()


class Linear(Module):
    """
    A module which applies a linear transformation 
    A common name is fully-connected layer, InnerProductLayer in caffe.

    The module should work with 2D input of shape (n_samples, n_feature).
    """

    def __init__(self, n_in, n_out):
        super(Linear, self).__init__()

        # This is a nice initialization
        stdv = 1. / np.sqrt(n_in)
        self.W = np.random.uniform(-stdv, stdv, size=(n_out, n_in))
        self.b = np.random.uniform(-stdv, stdv, size=n_out)

        self.gradW = np.zeros_like(self.W)
        self.gradb = np.zeros_like(self.b)

    def _compute_output(self, input):
        """
        Args:
            input: batch_size x n_feats1

        Returns:
            output: batch_size x n_feats2
        """

        return np.dot(input, self.W.T) + self.b

    def _compute_input_grad(self, input, output_grad):
        return np.dot(output_grad, self.W)

    def _update_parameters_grad(self, input, output_grad):
        self.gradW = np.dot(output_grad.T, input)
        self.gradb = np.sum(output_grad, axis=0)

    def zero_grad(self):
        self.gradW.fill(0)
        self.gradb.fill(0)

    def get_parameters(self):
        return [self.W, self.b]

    def get_parameters_grad(self):
        return [self.gradW, self.gradb]

    def __repr__(self):
        s = self.W.shape
        q = 'Linear %d -> %d' %(s[1], s[0])
        return q


X = np.array([[1, 2, 3, 4], [1, 1, 1, 1]])
m = Linear(n_in=4, n_out=2)
m.forward(X)


class ReLU(Module):
    def __init__(self):
         super(ReLU, self).__init__()

    def _compute_output(self, input):
        output = np.maximum(input, 0)
        return output

    def _compute_input_grad(self, input, output_grad):
        grad_input = np.multiply(output_grad , input > 0)
        return grad_input

    def __repr__(self):
        return "ReLU"


class Tanh(Module):
    def __init__(self):
         super(Tanh, self).__init__()

    def __repr__(self):
        return "Tanh"

    def __tanh(self, X):
        y1, y2 = np.exp(X), np.exp(-X) 
        return (y1 - y2) / (y1 + y2)

    def _compute_output(self, input):
        return self.__tanh(input)

    def _compute_input_grad(self, input, output_grad):
        return np.multiply(output_grad, 1 - np.square(self.__tanh(input)))


class Criterion:
    def __init__ (self):
        self._output = None
        self._input_grad = None
        
    def forward(self, input, target):
        """
        Given an input and a target, compute the loss function 
        associated to the criterion and return the result.
        
        For consistency this function should not be overrided,
        all the code goes in `_compute_output`.
        """
        self._output = self._compute_output(input, target)
        return self._output

    def backward(self, input, target):
        """
            Given an input and a target, compute the gradients of the loss function
            associated to the criterion and return the result. 

            For consistency this function should not be overrided,
            all the code goes in `_compute_input_grad`.
        """
        self._input_grad = self._compute_input_grad(input, target)
        return self._input_grad
    
    def _compute_output(self, input, target):
        """
        Function to override.
        """
        raise NotImplementedError

    def _compute_input_grad(self, input, target):
        """
        Returns gradient of input wrt output
        
        Function to override.
        """
        raise NotImplementedError

    def __repr__(self):
        """
        Pretty printing. Should be overrided in every module if you want 
        to have readable description. 
        """
        return "Criterion"
    

class NegativeLogLikelihood(Criterion):
    pass


class Optimizer:
    def __init__(self, network):
        self._network = network  # contains trainable paramenters and their gradients
        self._state = {}  # any information needed to save between optimizer iterations

    def step(self):
        """
        Updates network parameters
        """
        raise NotImplementedError


class SGD(Optimizer):
    def __init__(self, network, lr, momentum=0.9):
        super(SGD, self).__init__(network)
        self._learning_rate = lr
        self._momentum = momentum

    def step(self):
        variables = self._network.get_parameters()
        gradients = self._network.get_parameters_grad()

        # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one
        self._state.setdefault('accumulated_grads', {})

        var_index = 0 
        for current_layer_vars, current_layer_grads in zip(variables, gradients):
            for current_var, current_grad in zip(current_layer_vars, current_layer_grads):
                old_grad = self._state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))
                np.add(self._momentum * old_grad, current_grad, out=old_grad)
                current_var -= self._learning_rate * old_grad
                var_index += 1


seq = Sequential()
seq.add_module(
    Linear(n_in=4, n_out=2)
)
seq.add_module(
    ReLU()
)
seq.add_module(
    Linear(n_in=2, n_out=1)
)
seq.add_module(
    Tanh()
)

seq.evaluate()
seq.forward(X)


import torch


def get_backbone(i_dim, scale_bounded=False):
    stack = [
        Linear(i_dim, i_dim),
        ReLU(),
        Linear(i_dim, i_dim),
        ReLU(),
        Linear(i_dim, i_dim),
    ]

    if scale_bounded:
        stack.append(Tanh())

    seq = Sequential()
    for part in stack:
        seq.add_module(part)
    return seq


class RealNVPBlock:
    def __init__(self, input_dim, reverse=False):

        self.input_dim = input_dim
        mid = int(input_dim / 2)

        self.S = get_backbone(mid, scale_bounded=True)
        self.T = get_backbone(mid)

    def generation(self, y):
        y1, y2 = y[:, :self.input_dim // 2], y[:, self.input_dim // 2:]

        scale = self.S.forward(y1)
        shift = self.T.forward(y1)

        x1 = y1
        x2 = (y2 - shift) * np.exp(-scale)

        x = np.concatenate([x1, x2], axis=1)
        return x

    def inference(self, x):
        x1, x2 = x[:, :self.input_dim // 2], x[:, self.input_dim // 2:]

        scale = self.S.forward(x1)
        shift = self.T.forward(x1)

        y1 = x1
        y2 = x2 * np.exp(scale) + shift

        z = np.concatenate([y1, y2], axis=1)
        log_det_jacobian = scale.sum(axis=1)

        return z, log_det_jacobian


import torch
import torch.nn as nn


class RealNVP(nn.Module):
    def __init__(self, size, n_layers):
        super().__init__()

        # Define the prior distribution (e.g., a standard normal distribution)
        self.prior = torch.distributions.Normal(torch.zeros(size), torch.ones(size))

        # Initialize RealNVPBlock layers
        self.layers = nn.ModuleList([RealNVPBlock(size) for _ in range(n_layers)])

    def generation(self, z):
        x = z
        for layer in self.layers:
            x, _ = layer.forward(x)
        return x

    def inference(self, x):
        log_det_jacobian = 0.
        z = x
        for layer in reversed(self.layers):
            z, log_det_j = layer.inverse(z)
            log_det_jacobian += log_det_j
        return z, log_det_jacobian

    def sample(self, n_samples):
        z = self.prior.sample((n_samples,))
        x = self.generation(z)
        return x

    def negative_log_likelihood(self, x):
        z, log_det_jacobian = self.inference(x)
        log_prior = self.prior.log_prob(z).sum(dim=1)
        nll = - (log_prior + log_det_jacobian).mean()
        return nll



class RealNVP:
    def __init__(self, size, n_layers):
        super().__init__()
        
        self.prior = ...  # Recall about NLL from first part
        self.layers = ...  # Sequence of RealNVPBlock-s
    
    def generation(self, z):
        x = z
        for i in range(len(self.layers)):
            pass
        return x
    
    def inference(self, x):
        log_det_jacobian = 0.
        z = x
        for i in reversed(range(len(self.layers))):
            pass

            # remember here, we just have to sum all log det jacobians!
        return z, log_det_jacobian

    
    def sample(self, n_samples):
        z = ...
        x = ...
        return x

    def negative_log_likelihood(self, x):
        # (3) formula from RealNVP paper
        pass


from scipy.optimize import bisect
import scipy.integrate as integrate
import matplotlib.pyplot as plt

def pdf(x, sigma=2, M=1):
    return 1/(sigma*np.sqrt(2*np.pi)) * np.exp(-(x-M)**2/(2*sigma**2))

def cdf(x):
    return integrate.quad(pdf, -np.inf, x)[0]

def inverse_cdf(p, lower_bound, upper_bound, tolerance=1e-5):
    def root_function(x):
        return cdf(x) - p

    return bisect(root_function, lower_bound, upper_bound, xtol=tolerance)


x = np.linspace(-10, 10, 100)
y = raspred(x)

y_d = [cdf(x) for x in x]

plt.plot(x, y)
plt.plot(x, y_d)

plt.show()


y_d


cdf(x)



